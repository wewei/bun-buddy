import OpenAI from 'openai';
import { randomUUID } from 'crypto';

// åŸºç¡€ç±»å‹å®šä¹‰
export type ChatMessage = {
  role: 'user' | 'assistant' | 'system';
  content: string;
};

export type CompletionChunk = {
  trackingId: string;
  content: string;
  finished: boolean;
  error?: string;
};

export type LLMConfig = {
  endpoint: {
    url: string;
    key: string;
    model: string;
  };
  temperature?: number;
  maxTokens?: number;
  tools?: OpenAI.Chat.Completions.ChatCompletionTool[];
  stream?: boolean;
};

// é…ç½®å·¥å‚å‡½æ•°
export const createLLMConfig = (config: Partial<LLMConfig> & { endpoint: LLMConfig['endpoint'] }): LLMConfig => ({
  temperature: 0.7,
  maxTokens: 2000,
  stream: true,
  ...config
});

// éªŒè¯é…ç½®
export const validateLLMConfig = (config: LLMConfig): void => {
  if (!config.endpoint.url) {
    throw new Error('LLM endpoint URL is required');
  }
  if (!config.endpoint.key) {
    throw new Error('LLM API key is required');
  }
  if (!config.endpoint.model) {
    throw new Error('LLM model is required');
  }
};

// åˆ›å»º OpenAI å®¢æˆ·ç«¯
export const createOpenAIClient = (config: LLMConfig): OpenAI => {
  validateLLMConfig(config);
  return new OpenAI({
    apiKey: config.endpoint.key,
    baseURL: config.endpoint.url
  });
};

// è½¬æ¢æ¶ˆæ¯æ ¼å¼
export const convertToOpenAIMessages = (messages: ChatMessage[]): OpenAI.Chat.Completions.ChatCompletionMessageParam[] =>
  messages.map(msg => ({
    role: msg.role,
    content: msg.content
  }));

// ç”Ÿæˆè¿½è¸ª ID
export const generateTrackingId = (): string => randomUUID();

// åˆ›å»ºå®Œæˆå—
export const createCompletionChunk = (
  trackingId: string,
  content: string,
  finished: boolean = false,
  error?: string
): CompletionChunk => ({
  trackingId,
  content,
  finished,
  ...(error && { error })
});

// ä¸» LLM æµå¼è¯·æ±‚å‡½æ•°
export const streamLLMCompletion = async function* (
  config: LLMConfig,
  messages: ChatMessage[],
  trackingId?: string
): AsyncGenerator<CompletionChunk, void, unknown> {
  const id = trackingId || generateTrackingId();
  const client = createOpenAIClient(config);
  const openaiMessages = convertToOpenAIMessages(messages);

  try {
    console.log(`ğŸ¤– Starting LLM completion request with tracking ID: ${id}`);

    const stream = await client.chat.completions.create({
      model: config.endpoint.model,
      messages: openaiMessages,
      stream: config.stream ?? true,
      temperature: config.temperature,
      max_tokens: config.maxTokens,
      ...(config.tools && { tools: config.tools })
    }) as AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>;

    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta;
      const content = delta?.content;
      
      if (content) {
        yield createCompletionChunk(id, content, false);
      }
    }

    // å‘é€å®Œæˆä¿¡å·
    yield createCompletionChunk(id, '', true);
    
  } catch (error) {
    console.error(`ğŸ¤– LLM completion error (${id}):`, error);
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    yield createCompletionChunk(id, '', true, errorMessage);
  }
};

// è¾…åŠ©å‡½æ•°ï¼šä»é…ç½®åˆ›å»º LLM é…ç½®
export const createLLMConfigFromEndpoint = (
  endpoint: { url: string; key: string; model: string },
  options?: Partial<LLMConfig>
): LLMConfig => createLLMConfig({
  endpoint,
  ...options
});

// è¾…åŠ©å‡½æ•°ï¼šå¤„ç†æµå¼å“åº”çš„å›è°ƒ
export const withCallback = async function* (
  stream: AsyncGenerator<CompletionChunk, void, unknown>,
  onChunk?: (chunk: CompletionChunk) => void
): AsyncGenerator<CompletionChunk, void, unknown> {
  for await (const chunk of stream) {
    if (onChunk) {
      onChunk(chunk);
    }
    yield chunk;
  }
};


/*
ä½¿ç”¨ç¤ºä¾‹ï¼š

// 1. åŸºæœ¬ç”¨æ³•
const config = createLLMConfig({
  endpoint: {
    url: 'https://api.openai.com/v1',
    key: 'your-api-key',
    model: 'gpt-4'
  },
  temperature: 0.8,
  maxTokens: 1000
});

const messages: ChatMessage[] = [
  { role: 'user', content: 'Hello, how are you?' }
];

// æµå¼å¤„ç†
for await (const chunk of streamLLMCompletion(config, messages)) {
  if (chunk.finished) {
    console.log('Completion finished');
    if (chunk.error) {
      console.error('Error:', chunk.error);
    }
  } else {
    process.stdout.write(chunk.content);
  }
}

// 2. å¸¦å›è°ƒçš„ç”¨æ³•
const stream = streamLLMCompletion(config, messages);
const streamWithCallback = withCallback(stream, (chunk) => {
  console.log('Received chunk:', chunk);
});

for await (const chunk of streamWithCallback) {
  // å¤„ç† chunk
}

// 3. ä»ç°æœ‰é…ç½®åˆ›å»º
const endpoint = { url: 'https://api.openai.com/v1', key: 'key', model: 'gpt-4' };
const configFromEndpoint = createLLMConfigFromEndpoint(endpoint, {
  temperature: 0.5,
  tools: [] // ä½ çš„å·¥å…·æ•°ç»„
});
*/