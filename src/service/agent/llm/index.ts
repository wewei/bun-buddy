import OpenAI from 'openai';
import { randomUUID } from 'crypto';
import type { 
  ChatMessage, 
  CompletionChunk, 
  LLMConfig, 
  ToolCall,
  ToolDefinition
} from '../types';

// å¯¼å‡ºç±»å‹ä¾›å¤–éƒ¨ä½¿ç”¨
export type { 
  ChatMessage, 
  CompletionChunk, 
  LLMConfig, 
  ToolCall,
  ToolDefinition 
};

// é…ç½®å·¥å‚å‡½æ•°
export const createLLMConfig = (config: Partial<LLMConfig> & { endpoint: LLMConfig['endpoint'] }): LLMConfig => ({
  temperature: 0.7,
  maxTokens: 2000,
  stream: true,
  ...config
});

// éªŒè¯é…ç½®
export const validateLLMConfig = (config: LLMConfig): void => {
  if (!config.endpoint.url) {
    throw new Error('LLM endpoint URL is required');
  }
  if (!config.endpoint.key) {
    throw new Error('LLM API key is required');
  }
  if (!config.endpoint.model) {
    throw new Error('LLM model is required');
  }
};

// åˆ›å»º OpenAI å®¢æˆ·ç«¯
export const createOpenAIClient = (config: LLMConfig): OpenAI => {
  validateLLMConfig(config);
  return new OpenAI({
    apiKey: config.endpoint.key,
    baseURL: config.endpoint.url
  });
};


// ç”Ÿæˆè¿½è¸ª ID
export const generateTrackingId = (): string => randomUUID();

// åˆ›å»ºå®Œæˆå—ï¼ˆæ‰©å±•æ”¯æŒ tool callsï¼‰
export const createCompletionChunk = (
  trackingId: string,
  content: string,
  finished: boolean = false,
  error?: string,
  toolCalls?: ToolCall[]
): CompletionChunk => ({
  trackingId,
  content,
  finished,
  ...(error && { error }),
  ...(toolCalls && { toolCalls })
});


// åˆ›å»º OpenAI æµå¼è¯·æ±‚å‚æ•°
const createStreamParams = (
  config: LLMConfig,
  messages: ChatMessage[]
): OpenAI.Chat.Completions.ChatCompletionCreateParamsStreaming => ({
  model: config.endpoint.model,
  messages,
  stream: true,
  temperature: config.temperature,
  max_tokens: config.maxTokens,
  ...(config.tools && { tools: config.tools })
});

// ç´¯ç§¯ tool callsï¼ˆå¤„ç†æµå¼ tool calls çš„å¢é‡æ›´æ–°ï¼‰
type ToolCallAccumulator = Map<number, { id: string; name: string; args: string }>;

const accumulateToolCall = (
  acc: ToolCallAccumulator,
  toolCall: OpenAI.Chat.Completions.ChatCompletionChunk.Choice.Delta.ToolCall
): ToolCallAccumulator => {
  const index = toolCall.index;
  const existing = acc.get(index) || { id: '', name: '', args: '' };
  
  acc.set(index, {
    id: toolCall.id || existing.id,
    name: toolCall.function?.name || existing.name,
    args: existing.args + (toolCall.function?.arguments || '')
  });
  
  return acc;
};

// è½¬æ¢ç´¯ç§¯çš„ tool calls ä¸ºæœ€ç»ˆæ ¼å¼
const finalizeToolCalls = (acc: ToolCallAccumulator): ToolCall[] =>
  Array.from(acc.values())
    .filter(tc => tc.id && tc.name && tc.args)
    .map(tc => ({
      id: tc.id,
      type: 'function' as const,
      function: {
        name: tc.name,
        arguments: tc.args
      }
    }));

// ä¸» LLM æµå¼è¯·æ±‚å‡½æ•°
export const streamLLMCompletion = async function* (
  config: LLMConfig,
  messages: ChatMessage[],
  trackingId?: string
): AsyncGenerator<CompletionChunk, void, unknown> {
  const id = trackingId || generateTrackingId();
  const client = createOpenAIClient(config);
  const toolCallAcc: ToolCallAccumulator = new Map();

  try {
    console.log(`ğŸ¤– Starting LLM completion with tracking ID: ${id}`);
    
    const params = createStreamParams(config, messages);
    const stream = await client.chat.completions.create(params);

    for await (const chunk of stream) {
      const delta = chunk.choices[0]?.delta;
      
      // å¤„ç†æ–‡æœ¬å†…å®¹
      if (delta?.content) {
        yield createCompletionChunk(id, delta.content, false);
      }
      
      // ç´¯ç§¯ tool calls
      if (delta?.tool_calls) {
        delta.tool_calls.forEach(tc => accumulateToolCall(toolCallAcc, tc));
      }
    }

    // å‘é€å®Œæˆä¿¡å·ï¼ˆåŒ…å«æœ€ç»ˆçš„ tool callsï¼‰
    const finalToolCalls = finalizeToolCalls(toolCallAcc);
    yield createCompletionChunk(
      id, 
      '', 
      true, 
      undefined, 
      finalToolCalls.length > 0 ? finalToolCalls : undefined
    );
    
  } catch (error) {
    console.error(`ğŸ¤– LLM completion error (${id}):`, error);
    const errorMessage = error instanceof Error ? error.message : 'Unknown error';
    yield createCompletionChunk(id, '', true, errorMessage);
  }
};

// è¾…åŠ©å‡½æ•°ï¼šä»é…ç½®åˆ›å»º LLM é…ç½®
export const createLLMConfigFromEndpoint = (
  endpoint: { url: string; key: string; model: string },
  options?: Partial<LLMConfig>
): LLMConfig => createLLMConfig({
  endpoint,
  ...options
});

// è¾…åŠ©å‡½æ•°ï¼šå¤„ç†æµå¼å“åº”çš„å›è°ƒ
export const withCallback = async function* (
  stream: AsyncGenerator<CompletionChunk, void, unknown>,
  onChunk?: (chunk: CompletionChunk) => void
): AsyncGenerator<CompletionChunk, void, unknown> {
  for await (const chunk of stream) {
    if (onChunk) {
      onChunk(chunk);
    }
    yield chunk;
  }
};


/*
ä½¿ç”¨ç¤ºä¾‹ï¼š

// 1. åŸºæœ¬ç”¨æ³•
const config = createLLMConfig({
  endpoint: {
    url: 'https://api.openai.com/v1',
    key: 'your-api-key',
    model: 'gpt-4'
  },
  temperature: 0.8,
  maxTokens: 1000
});

const messages: ChatMessage[] = [
  { role: 'user', content: 'Hello, how are you?' }
];

// æµå¼å¤„ç†
for await (const chunk of streamLLMCompletion(config, messages)) {
  if (chunk.finished) {
    console.log('Completion finished');
    if (chunk.error) {
      console.error('Error:', chunk.error);
    }
    if (chunk.toolCalls) {
      console.log('Tool calls:', chunk.toolCalls);
    }
  } else {
    process.stdout.write(chunk.content);
  }
}

// 2. å¸¦ tool definitions çš„ç”¨æ³•
const configWithTools = createLLMConfig({
  endpoint: { url: 'https://api.openai.com/v1', key: 'key', model: 'gpt-4' },
  tools: [
    {
      type: 'function',
      function: {
        name: 'search_web',
        description: 'Search the web for information',
        parameters: {
          type: 'object',
          properties: {
            query: { type: 'string', description: 'Search query' }
          },
          required: ['query']
        }
      }
    }
  ]
});

// 3. å¸¦å›è°ƒçš„ç”¨æ³•
const stream = streamLLMCompletion(config, messages);
const streamWithCallback = withCallback(stream, (chunk) => {
  console.log('Received chunk:', chunk);
});

for await (const chunk of streamWithCallback) {
  // å¤„ç† chunk
}
*/

